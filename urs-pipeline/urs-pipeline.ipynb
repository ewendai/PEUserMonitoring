{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import json\n",
    "from datetime import datetime\n",
    "import json\n",
    "from os import listdir\n",
    "from os.path import isfile, join"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 0. Grab data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "datahunt = pd.read_csv('../IAA-consensus/evidence_eric/Covid_Evidencev1-Task-2224-DataHunt.csv')\n",
    "schema = pd.read_csv('../IAA-consensus/evidence_eric/45dce5251bd3ea6e908fa33ac9e6a8e17e6830215912ce1626cf4206e159819c.csv')\n",
    "iaa = pd.read_csv('../IAA-consensus/evidence_eric/Covid_Evidencev1.IAA-edb1510f-1923-4d6f-a678-95f53d752bea-Tags.csv')\n",
    "adj = pd.read_csv('../IAA-consensus/evidence_eric/Covid_Evidence2020_03_21.adjudicated-edb1510f-1923-4d6f-a678-95f53d752bea-Tags.csv')\n",
    "#preprocessing = pd.read_csv('../urs-preprocessing/datahunt_tracking.json')\n",
    "\n",
    "with open('../urs-preprocessing/datahunt_tracking.json', 'r') as fp:\n",
    "    preprocessing = json.load(fp)\n",
    "\n",
    "#urs = pd.read_csv('../User Rep Score/score.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1. Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "datahunt[\"finish_time_object\"] = datahunt[\"finish_time\"].apply(datetime.fromisoformat)\n",
    "datahunt = datahunt.sort_values(\"finish_time_object\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "datahunt_name_regex = re.compile('[^-]*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this errors because of problems with getting the name of datahunts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# if len(datahunt) >= preprocessing[re.search(datahunt_name_regex, 'Covid_Evidencev1-Task-2224-DataHunt.csv').group()]:\n",
    "#     print(\"No unprocessed rows in this datahunt csv\")\n",
    "# else:\n",
    "#     datahunt = datahunt.iloc[preprocessing[re.search(datahunt_name_regex, 'Covid_Evidencev1-Task-2224-DataHunt.csv').group()]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2. Add IAA and adjudicated columns to datahunt csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "answers_iaa = iaa.merge(schema, how=\"inner\", on=\"answer_uuid\")\n",
    "answers_adj = adj.merge(schema, how=\"inner\", on=\"answer_uuid\")\n",
    "\n",
    "# filter down IAA tags file, replace nan values to prevent errors\n",
    "answers_iaa = answers_iaa[[\"answer_uuid\", \"source_task_uuid\", \"tua_uuid\",\n",
    "                           \"target_text\", \"question_label\", \"answer_label\",\n",
    "                           \"question_type_x\", \"question_type_y\", \"answer_count\",\n",
    "                           \"alpha_distance\"]]\n",
    "\n",
    "answers_iaa = answers_iaa.replace(np.nan, '', regex=True)\n",
    "\n",
    "# filter down adjudicated/gold standard tags file, replace nan values to prevent errors\n",
    "answers_adj = answers_adj[[\"answer_uuid\", \"source_task_uuid\", \"tua_uuid\",\n",
    "                           \"target_text\", \"question_label\", \"answer_label\",\n",
    "                           \"question_type\", \"answer_count\", \"alpha_distance\"]]\n",
    "\n",
    "answers_adj = answers_adj.replace(np.nan, '', regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_consensus(answers, question_label, quiz_task_uuid):\n",
    "    answer_df = answers.loc[(answers[\"question_label\"] == question_label)\n",
    "                         & (answers[\"source_task_uuid\"] == quiz_task_uuid)]\n",
    "    \n",
    "    return list(set(answer_df[\"answer_label\"].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "datahunt['iaa_consensus'] = datahunt.apply(lambda x: get_consensus(answers_iaa, x['question_label'], x['quiz_task_uuid']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "datahunt['adj_consensus'] = datahunt.apply(lambda x: get_consensus(answers_adj, x['question_label'], x['quiz_task_uuid']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_question_meta(answers_iaa, answers_adj, question_label, quiz_task_uuid):\n",
    "    answer_iaa = answers_iaa.loc[(answers_iaa[\"question_label\"] == question_label)\n",
    "                         & (answers_iaa[\"source_task_uuid\"] == quiz_task_uuid)]\n",
    "    \n",
    "    answer_adj = answers_adj.loc[(answers_adj[\"question_label\"] == question_label)\n",
    "                         & (answers_adj[\"source_task_uuid\"] == quiz_task_uuid)]\n",
    "    \n",
    "    if len(answer_iaa[\"question_type_y\"]) > 0 and len(answer_iaa[\"answer_count\"]) > 0:\n",
    "        question_type_y = answer_iaa[\"question_type_y\"].iloc[0]\n",
    "        num_answer_choices = answer_iaa[\"answer_count\"].iloc[0]\n",
    "\n",
    "        if question_type_y == \"RADIO\":\n",
    "            question_type_y = answer_iaa[\"alpha_distance\"].iloc[0].upper()\n",
    "\n",
    "        return (question_type_y, num_answer_choices)\n",
    "    \n",
    "    elif len(answer_adj[\"question_type\"]) > 0 and len(answer_adj[\"answer_count\"]) > 0:\n",
    "        question_type = answer_adj[\"question_type\"].iloc[0]\n",
    "        num_answer_choices = answer_adj[\"answer_count\"].iloc[0]\n",
    "\n",
    "        if question_type == \"RADIO\":\n",
    "            question_type = answer_adj[\"alpha_distance\"].iloc[0].upper()\n",
    "\n",
    "        return (question_type, num_answer_choices)\n",
    "    \n",
    "    else:\n",
    "        return ()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "datahunt['question_meta'] = datahunt.apply(lambda x: get_question_meta(answers_iaa, answers_adj,\n",
    "                                                                       x['question_label'],\n",
    "                                                                       x['quiz_task_uuid']), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3. Update user rep scores "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_sum(series):\n",
    "    ret = set()\n",
    "    for s in series:\n",
    "        ret = ret.union(s)\n",
    "    return ret\n",
    "\n",
    "def into_set(series):\n",
    "    ret = set()\n",
    "    for s in series:\n",
    "        ret.add(s)\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = datahunt\n",
    "#data['adj_consensus'] = data['adj_consensus'].str.replace('\\'', '').str.replace('[', '').str.replace(']', '').str.split(\", \").apply(lambda x: set(x))\n",
    "#data['iaa_consensus'] = data['iaa_consensus'].str.replace('\\'', '').str.replace('[', '').str.replace(']', '').str.split(\", \").apply(lambda x: set(x))\n",
    "\n",
    "\n",
    "#Reviewers' current reputation score\n",
    "old_rep_scores = pd.read_csv(\"../User Rep Score/score.csv\").set_index('contributor_uuid')\n",
    "old_rep_scores = {user: old_rep_scores.loc[user, \"score\"] if user in old_rep_scores.index else 0.5 \n",
    "              for user in data[\"contributor_uuid\"].unique()}\n",
    "rep_scores = {user:0 for user in old_rep_scores.keys()}\n",
    "\n",
    "#Produce pivot tables we need for calculating consensus\n",
    "adj_table = pd.pivot_table(data, values='adj_consensus', index='article_number', columns='question_label', aggfunc=set_sum)\n",
    "iaa_table = pd.pivot_table(data, values='iaa_consensus', index='article_number', columns='question_label', aggfunc=set_sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.pivot_table(data,\n",
    "                    values='answer_label', \n",
    "                    index='contributor_uuid', \n",
    "                    columns='question_label', \n",
    "                    aggfunc=into_set)\n",
    "for user in df.index:\n",
    "    review = df.loc[df.index == user, :]\n",
    "    n_adj_answers = 0\n",
    "    n_correct = 0\n",
    "    for question_label in df.columns:\n",
    "        if question_label in adj_table.columns and len(adj_table[question_label].iloc[0]) > 0:\n",
    "            n_adj_answers += 1\n",
    "            adj_ans = adj_table[question_label].iloc[0]\n",
    "            user_ans = review.loc[:, question_label].iloc[0]\n",
    "            if type(user_ans) is set and len(user_ans.intersection(adj_ans)) > 0:\n",
    "                n_correct += 1\n",
    "    rep_scores[user] = n_correct/n_adj_answers\n",
    "                \n",
    "for user in rep_scores.keys():\n",
    "    rep_scores[user] = 0.5 * rep_scores[user] + 0.5 * old_rep_scores[user]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_rep_scores = rep_scores\n",
    "rep_scores = {user:[] for user in old_rep_scores.keys()}\n",
    "\n",
    "df = pd.pivot_table(data,\n",
    "                    values='answer_label', \n",
    "                    index='contributor_uuid', \n",
    "                    columns='question_label', \n",
    "                    aggfunc=into_set)\n",
    "for user in df.index:\n",
    "    review = df.loc[df.index == user, :]\n",
    "    n_iaa_answers = 0\n",
    "    n_correct = 0\n",
    "    for question_label in df.columns:\n",
    "        if question_label in iaa_table.columns and len(iaa_table[question_label].iloc[0]) > 0:\n",
    "            n_iaa_answers += 1\n",
    "            iaa_ans = iaa_table[question_label].iloc[0]\n",
    "            user_ans = review.loc[:, question_label].iloc[0]\n",
    "            if type(user_ans) is set and len(user_ans.intersection(iaa_ans)) > 0:\n",
    "                n_correct += 1\n",
    "    rep_scores[user].append(n_correct/n_iaa_answers)\n",
    "                \n",
    "for user in rep_scores.keys():\n",
    "    rep_scores[user] = 0.3 * sum(rep_scores[user])/len(rep_scores[user]) + 0.7 * old_rep_scores[user]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update the repscore csv file\n",
    "csv = pd.read_csv(\"../User Rep Score/score.csv\").set_index('contributor_uuid')\n",
    "for user in rep_scores.keys():\n",
    "    if user in csv.index:\n",
    "        csv.loc[user, 'score'] = rep_scores[user]\n",
    "    else:\n",
    "        helper_dict = {user:rep_scores[user]}\n",
    "        helper_df = pd.DataFrame.from_dict(helper_dict, orient=\"index\")\n",
    "        helper_df.columns = ['score']\n",
    "        csv = csv.append(helper_df)\n",
    "csv = csv.reset_index()\n",
    "csv.columns = ['contributor_uuid', 'score']\n",
    "csv.to_csv(\"../User Rep Score/score.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4. Update preprocessing csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if datahunt_name_regex in preprocessing:\n",
    "#         file_size = len(pd.read_csv(join(\"../IAA-consensus/evidence_eric/\", 'Covid_Evidencev1-Task-2224-DataHunt.csv')))\n",
    "#         preprocessing[datahunt_name_regex] = max(preprocessing[datahunt_name_regex], file_size)\n",
    "# else:\n",
    "#         file_size = len(pd.read_csv(join(\"../IAA-consensus/evidence_eric/\", 'Covid_Evidencev1-Task-2224-DataHunt.csv')))\n",
    "#         preprocessing[datahunt_name_regex] = file_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
