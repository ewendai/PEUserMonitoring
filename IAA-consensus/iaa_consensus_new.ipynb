{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib as plt\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Data from Eric, all four of these files confirmed correspond to each other\n",
    "datahunt = pd.read_csv('evidence_eric/Covid_Evidencev1-Task-2224-DataHunt.csv')\n",
    "schema = pd.read_csv('evidence_eric/45dce5251bd3ea6e908fa33ac9e6a8e17e6830215912ce1626cf4206e159819c.csv')\n",
    "iaa = pd.read_csv('evidence_eric/Covid_Evidencev1.IAA-edb1510f-1923-4d6f-a678-95f53d752bea-Tags.csv')\n",
    "adj = pd.read_csv('evidence_eric/Covid_Evidence2020_03_21.adjudicated-edb1510f-1923-4d6f-a678-95f53d752bea-Tags.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The iaa (IAA, inter-annotator agreement) and adj (adjudicated, Gold-Standard) files are tag files, and each contain a row for each converged answer to a given task. The iaa file has converged answers based on regular user responses on tasks, while the adj file has answers based on an experience Public Editor user (ex. Nick, Emlen, Eric).\n",
    "\n",
    "These tag files contain the answer_uuid for each converged answer (string of letters and numbers, not human-interpretable), but not the answer_label (ex. 'T1.Q1.A2') or information such as the question type. Instead, this additional data is stored in the schema files, which we can combine with the tag files by merging on the answer_uuid column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "answers_iaa = iaa.merge(schema, how=\"inner\", on=\"answer_uuid\")\n",
    "answers_adj = adj.merge(schema, how=\"inner\", on=\"answer_uuid\")\n",
    "\n",
    "# filter down IAA tags file, replace nan values to prevent errors\n",
    "answers_iaa = answers_iaa[[\"answer_uuid\", \"source_task_uuid\", \"tua_uuid\",\n",
    "                           \"target_text\", \"question_label\", \"answer_label\",\n",
    "                           \"question_type_x\", \"question_type_y\", \"answer_count\",\n",
    "                           \"alpha_distance\"]]\n",
    "\n",
    "answers_iaa = answers_iaa.replace(np.nan, '', regex=True)\n",
    "\n",
    "# filter down adjudicated/gold standard tags file, replace nan values to prevent errors\n",
    "answers_adj = answers_adj[[\"answer_uuid\", \"source_task_uuid\", \"tua_uuid\",\n",
    "                           \"target_text\", \"question_label\", \"answer_label\",\n",
    "                           \"question_type\", \"answer_count\", \"alpha_distance\"]]\n",
    "\n",
    "answers_adj = answers_adj.replace(np.nan, '', regex=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## get_consensus function\n",
    "Function that takes in  **question_label**, **source_task_uuid/quiz_task_uuid**, and **answer_uuid**, and returns a list of all the corresponding consensus answers. This could be an empty list if there is no consensus for that question, a length one list if the question is a \"select one\" question (including ordinal questions), or a list of multiple answers if it is a \"select one\"/checkbox question.\n",
    "\n",
    "If consensus answers exist, they will be in the form T1.Q_.A_\n",
    "\n",
    "The above three columns are a unique identifier for a (set of) consensus answer(s). \n",
    "- **question_label** is in the form T1.Q_\n",
    "- **source_task_uuid/quiz_task_uuid identifies** a unique combination of a schema, an article that schema is applied to, and the bolded text unit of analysis.  \n",
    "    - quiz_task_uuid and source_task_uuid are essentially the same, but quiz is the column name in datahunt csvs, and source is the column name in the “answer” csvs\n",
    "\n",
    "- Motivation: use something like \n",
    "```\n",
    "df['iaa_consensus'] = df.apply(lambda x: get_consensus(x['question_label'], x['quiz_task_uuid']), axis=1)\n",
    "```\n",
    "to add a column to the datahunt dataframe with the iaa consensus answers or gold standard answers (works the exact same way for both) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_consensus(answers, question_label, quiz_task_uuid):\n",
    "    answer_df = answers.loc[(answers[\"question_label\"] == question_label)\n",
    "                         & (answers[\"source_task_uuid\"] == quiz_task_uuid)]\n",
    "    \n",
    "    return list(set(answer_df[\"answer_label\"].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "datahunt['iaa_consensus'] = datahunt.apply(lambda x: get_consensus(answers_iaa, x['question_label'], x['quiz_task_uuid']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "datahunt['adj_consensus'] = datahunt.apply(lambda x: get_consensus(answers_adj, x['question_label'], x['quiz_task_uuid']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question_label</th>\n",
       "      <th>quiz_task_uuid</th>\n",
       "      <th>answer_label</th>\n",
       "      <th>iaa_consensus</th>\n",
       "      <th>adj_consensus</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>T1.Q1</td>\n",
       "      <td>edb1510f-1923-4d6f-a678-95f53d752bea</td>\n",
       "      <td>T1.Q1.A1</td>\n",
       "      <td>[T1.Q1.A1]</td>\n",
       "      <td>[T1.Q1.A1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>T1.Q2</td>\n",
       "      <td>edb1510f-1923-4d6f-a678-95f53d752bea</td>\n",
       "      <td>T1.Q2.A1</td>\n",
       "      <td>[T1.Q2.A3, T1.Q2.A1, T1.Q2.A5, T1.Q2.A2]</td>\n",
       "      <td>[T1.Q2.A3, T1.Q2.A1, T1.Q2.A5, T1.Q2.A2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>T1.Q2</td>\n",
       "      <td>edb1510f-1923-4d6f-a678-95f53d752bea</td>\n",
       "      <td>T1.Q2.A2</td>\n",
       "      <td>[T1.Q2.A3, T1.Q2.A1, T1.Q2.A5, T1.Q2.A2]</td>\n",
       "      <td>[T1.Q2.A3, T1.Q2.A1, T1.Q2.A5, T1.Q2.A2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>T1.Q2</td>\n",
       "      <td>edb1510f-1923-4d6f-a678-95f53d752bea</td>\n",
       "      <td>T1.Q2.A3</td>\n",
       "      <td>[T1.Q2.A3, T1.Q2.A1, T1.Q2.A5, T1.Q2.A2]</td>\n",
       "      <td>[T1.Q2.A3, T1.Q2.A1, T1.Q2.A5, T1.Q2.A2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>T1.Q4</td>\n",
       "      <td>edb1510f-1923-4d6f-a678-95f53d752bea</td>\n",
       "      <td>T1.Q4.A6</td>\n",
       "      <td>[T1.Q4.A4]</td>\n",
       "      <td>[T1.Q4.A2]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  question_label                        quiz_task_uuid answer_label  \\\n",
       "0          T1.Q1  edb1510f-1923-4d6f-a678-95f53d752bea     T1.Q1.A1   \n",
       "1          T1.Q2  edb1510f-1923-4d6f-a678-95f53d752bea     T1.Q2.A1   \n",
       "2          T1.Q2  edb1510f-1923-4d6f-a678-95f53d752bea     T1.Q2.A2   \n",
       "3          T1.Q2  edb1510f-1923-4d6f-a678-95f53d752bea     T1.Q2.A3   \n",
       "4          T1.Q4  edb1510f-1923-4d6f-a678-95f53d752bea     T1.Q4.A6   \n",
       "\n",
       "                              iaa_consensus  \\\n",
       "0                                [T1.Q1.A1]   \n",
       "1  [T1.Q2.A3, T1.Q2.A1, T1.Q2.A5, T1.Q2.A2]   \n",
       "2  [T1.Q2.A3, T1.Q2.A1, T1.Q2.A5, T1.Q2.A2]   \n",
       "3  [T1.Q2.A3, T1.Q2.A1, T1.Q2.A5, T1.Q2.A2]   \n",
       "4                                [T1.Q4.A4]   \n",
       "\n",
       "                              adj_consensus  \n",
       "0                                [T1.Q1.A1]  \n",
       "1  [T1.Q2.A3, T1.Q2.A1, T1.Q2.A5, T1.Q2.A2]  \n",
       "2  [T1.Q2.A3, T1.Q2.A1, T1.Q2.A5, T1.Q2.A2]  \n",
       "3  [T1.Q2.A3, T1.Q2.A1, T1.Q2.A5, T1.Q2.A2]  \n",
       "4                                [T1.Q4.A2]  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datahunt[['question_label', 'quiz_task_uuid', 'answer_label', 'iaa_consensus', 'adj_consensus']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice below that the iaa_consensus and adj_consensus columns are very similar, aside from some small differences. This is a good sign, because it means that the converged answers from user task responses are very similar to the task responses from very experiences users."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## get_question_meta function\n",
    "Function that takes in  **question_label** and **source_task_uuid/quiz_task_uuid**, and returns a **tuple containing the question type and number of answer choices, will help with scoring questions**.\n",
    "\n",
    "If the question_type and num_answer_choices works as expected, we shouldn't need to hardcode the question schema data anymore.\n",
    "\n",
    "Returned question type will be NOMINAL (select one), ORDINAL (select one), CHECKBOX (select all), or TEXT (short answer).\n",
    "\n",
    "- Motivation: use something like \n",
    "```\n",
    "df['question_meta'] = df.apply(lambda x: get_question_meta(answers_iaa, x['question_label'], x['quiz_task_uuid']), axis=1)\n",
    "```\n",
    "to add a column to the datahunt df with the question metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_question_meta(answers_iaa, answers_adj, question_label, quiz_task_uuid):\n",
    "    answer_iaa = answers_iaa.loc[(answers_iaa[\"question_label\"] == question_label)\n",
    "                         & (answers_iaa[\"source_task_uuid\"] == quiz_task_uuid)]\n",
    "    \n",
    "    answer_adj = answers_adj.loc[(answers_adj[\"question_label\"] == question_label)\n",
    "                         & (answers_adj[\"source_task_uuid\"] == quiz_task_uuid)]\n",
    "    \n",
    "    if len(answer_iaa[\"question_type_y\"]) > 0 and len(answer_iaa[\"answer_count\"]) > 0:\n",
    "        question_type_y = answer_iaa[\"question_type_y\"].iloc[0]\n",
    "        num_answer_choices = answer_iaa[\"answer_count\"].iloc[0]\n",
    "\n",
    "        if question_type_y == \"RADIO\":\n",
    "            question_type_y = answer_iaa[\"alpha_distance\"].iloc[0].upper()\n",
    "\n",
    "        return (question_type_y, num_answer_choices)\n",
    "    \n",
    "    elif len(answer_adj[\"question_type\"]) > 0 and len(answer_adj[\"answer_count\"]) > 0:\n",
    "        question_type = answer_adj[\"question_type\"].iloc[0]\n",
    "        num_answer_choices = answer_adj[\"answer_count\"].iloc[0]\n",
    "\n",
    "        if question_type == \"RADIO\":\n",
    "            question_type = answer_adj[\"alpha_distance\"].iloc[0].upper()\n",
    "\n",
    "        return (question_type, num_answer_choices)\n",
    "    \n",
    "    else:\n",
    "        return ()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "datahunt['question_meta'] = datahunt.apply(lambda x: get_question_meta(answers_iaa, answers_adj,\n",
    "                                                                       x['question_label'],\n",
    "                                                                       x['quiz_task_uuid']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question_label</th>\n",
       "      <th>quiz_task_uuid</th>\n",
       "      <th>answer_label</th>\n",
       "      <th>iaa_consensus</th>\n",
       "      <th>adj_consensus</th>\n",
       "      <th>question_meta</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>T1.Q1</td>\n",
       "      <td>edb1510f-1923-4d6f-a678-95f53d752bea</td>\n",
       "      <td>T1.Q1.A1</td>\n",
       "      <td>[T1.Q1.A1]</td>\n",
       "      <td>[T1.Q1.A1]</td>\n",
       "      <td>(CHECKBOX, 3)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>T1.Q2</td>\n",
       "      <td>edb1510f-1923-4d6f-a678-95f53d752bea</td>\n",
       "      <td>T1.Q2.A1</td>\n",
       "      <td>[T1.Q2.A3, T1.Q2.A1, T1.Q2.A5, T1.Q2.A2]</td>\n",
       "      <td>[T1.Q2.A3, T1.Q2.A1, T1.Q2.A5, T1.Q2.A2]</td>\n",
       "      <td>(CHECKBOX, 9)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>T1.Q2</td>\n",
       "      <td>edb1510f-1923-4d6f-a678-95f53d752bea</td>\n",
       "      <td>T1.Q2.A2</td>\n",
       "      <td>[T1.Q2.A3, T1.Q2.A1, T1.Q2.A5, T1.Q2.A2]</td>\n",
       "      <td>[T1.Q2.A3, T1.Q2.A1, T1.Q2.A5, T1.Q2.A2]</td>\n",
       "      <td>(CHECKBOX, 9)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>T1.Q2</td>\n",
       "      <td>edb1510f-1923-4d6f-a678-95f53d752bea</td>\n",
       "      <td>T1.Q2.A3</td>\n",
       "      <td>[T1.Q2.A3, T1.Q2.A1, T1.Q2.A5, T1.Q2.A2]</td>\n",
       "      <td>[T1.Q2.A3, T1.Q2.A1, T1.Q2.A5, T1.Q2.A2]</td>\n",
       "      <td>(CHECKBOX, 9)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>T1.Q4</td>\n",
       "      <td>edb1510f-1923-4d6f-a678-95f53d752bea</td>\n",
       "      <td>T1.Q4.A6</td>\n",
       "      <td>[T1.Q4.A4]</td>\n",
       "      <td>[T1.Q4.A2]</td>\n",
       "      <td>(NOMINAL, 6)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  question_label                        quiz_task_uuid answer_label  \\\n",
       "0          T1.Q1  edb1510f-1923-4d6f-a678-95f53d752bea     T1.Q1.A1   \n",
       "1          T1.Q2  edb1510f-1923-4d6f-a678-95f53d752bea     T1.Q2.A1   \n",
       "2          T1.Q2  edb1510f-1923-4d6f-a678-95f53d752bea     T1.Q2.A2   \n",
       "3          T1.Q2  edb1510f-1923-4d6f-a678-95f53d752bea     T1.Q2.A3   \n",
       "4          T1.Q4  edb1510f-1923-4d6f-a678-95f53d752bea     T1.Q4.A6   \n",
       "\n",
       "                              iaa_consensus  \\\n",
       "0                                [T1.Q1.A1]   \n",
       "1  [T1.Q2.A3, T1.Q2.A1, T1.Q2.A5, T1.Q2.A2]   \n",
       "2  [T1.Q2.A3, T1.Q2.A1, T1.Q2.A5, T1.Q2.A2]   \n",
       "3  [T1.Q2.A3, T1.Q2.A1, T1.Q2.A5, T1.Q2.A2]   \n",
       "4                                [T1.Q4.A4]   \n",
       "\n",
       "                              adj_consensus  question_meta  \n",
       "0                                [T1.Q1.A1]  (CHECKBOX, 3)  \n",
       "1  [T1.Q2.A3, T1.Q2.A1, T1.Q2.A5, T1.Q2.A2]  (CHECKBOX, 9)  \n",
       "2  [T1.Q2.A3, T1.Q2.A1, T1.Q2.A5, T1.Q2.A2]  (CHECKBOX, 9)  \n",
       "3  [T1.Q2.A3, T1.Q2.A1, T1.Q2.A5, T1.Q2.A2]  (CHECKBOX, 9)  \n",
       "4                                [T1.Q4.A2]   (NOMINAL, 6)  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datahunt[['question_label', 'quiz_task_uuid', 'answer_label', 'iaa_consensus', 'adj_consensus', 'question_meta']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "datahunt.to_csv(\"test/evidence_datahunt_with_consensus.csv\", encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
